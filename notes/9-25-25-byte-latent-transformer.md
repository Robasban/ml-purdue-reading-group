# Byte Latent Transformer
Presenter: Arnav Grover 
Date: 9/25/25

## Summary

## Notes
- Tokenization: splitting text up into units to be easier to process
  - Many languages do not have orthographic words (spaces between words)
  - The number of words in a language grows endlessly, so it is hard for a model to know a word after it has been trained
    - By using tokenization, the model can predict what a new word means
